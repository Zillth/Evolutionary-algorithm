{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zillth/Evolutionary-algorithm/blob/main/Red_Neurnal_MONIC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Técnicas para ampliar el dataset:"
      ],
      "metadata": {
        "id": "cGe3eoR5aWJ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Repetición de datos"
      ],
      "metadata": {
        "id": "fRZ5SKC0aS7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Cargar el CSV existente\n",
        "csv_path = 'facturas.csv'\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Definir la cantidad de veces que deseas repetir los datos\n",
        "replication_factor = 5  # Puedes ajustar este valor según tus necesidades\n",
        "\n",
        "# Repetir los datos en el DataFrame\n",
        "df_repeated = pd.concat([df] * replication_factor, ignore_index=True)\n",
        "\n",
        "# Guardar el DataFrame con datos repetidos\n",
        "df_repeated.to_csv('facturas_rep.csv', index=False)"
      ],
      "metadata": {
        "id": "ymN2hHNOaKTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creación de datos sintéticos"
      ],
      "metadata": {
        "id": "m_QLekthaLxP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faker"
      ],
      "metadata": {
        "id": "f_PtThKVZyI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from faker import Faker\n",
        "import random\n",
        "\n",
        "# Cargar el CSV existente\n",
        "csv_path = 'facturas.csv'\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Inicializar el generador de datos sintéticos\n",
        "faker = Faker()\n",
        "\n",
        "faker = Faker('es_ES')  # Establece el idioma español\n",
        "\n",
        "# Definir la cantidad de muestras sintéticas que deseas generar\n",
        "num_samples_to_generate = 1000\n",
        "\n",
        "# Función para generar una fila de datos sintéticos\n",
        "def generate_synthetic_row():\n",
        "    return {\n",
        "        'Cliente': faker.company(),\n",
        "        'Fecha': faker.date_this_decade(),\n",
        "        'Monto': random.uniform(1, 1000),\n",
        "        # INCOMPLETO\n",
        "    }\n",
        "\n",
        "# Generar datos sintéticos y agregarlos al DataFrame\n",
        "for _ in range(num_samples_to_generate):\n",
        "    synthetic_row = generate_synthetic_row()\n",
        "    df = df.append(synthetic_row, ignore_index=True)\n",
        "\n",
        "# Guardar el DataFrame actualizado con las muestras sintéticas\n",
        "df.to_csv('facturas_fake.csv', index=False)"
      ],
      "metadata": {
        "id": "3358MlDcZuL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Datos sinteticos usando Transformadores"
      ],
      "metadata": {
        "id": "cK83FWypawwk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers torch"
      ],
      "metadata": {
        "id": "SsznJ9sKav4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "\n",
        "# Cargar modelo pre-entrenado y tokenizador\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "# Cantidad de muestras sintéticas que deseas generar\n",
        "num_samples_to_generate = 1000\n",
        "\n",
        "# Función para generar texto sintético\n",
        "def generate_synthetic_text(prompt=\"\"):\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "    output = model.generate(input_ids, max_length=50, num_return_sequences=1)\n",
        "    synthetic_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return synthetic_text\n",
        "\n",
        "# Generar texto sintético y agregarlo al conjunto de datos\n",
        "for _ in range(num_samples_to_generate):\n",
        "    synthetic_text = generate_synthetic_text()\n",
        "    # Agregar synthetic_text a tu conjunto de datos como sea apropiado INCOMPLETO\n",
        "\n",
        "# Guardar el conjunto de datos actualizado con datos sintéticos\n",
        "df.to_csv('facturas.csv', index=False)"
      ],
      "metadata": {
        "id": "rwdvuC-SazWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Red Neuronal"
      ],
      "metadata": {
        "id": "1QlOr87UZ9if"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8GAR0Q_Xprb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Cargar datos desde CSV\n",
        "facturas_data = pd.read_csv('facturas.csv')\n",
        "movimientos_data = pd.read_csv('movimientos.csv')\n",
        "\n",
        "# Combinar datos\n",
        "merged_data = pd.merge(facturas_data, movimientos_data, how='inner', on='ID_comun')\n",
        "\n",
        "# Separar características (X) y etiquetas (y)\n",
        "X = merged_data.drop(['ID_comun', 'fecha', 'nombre', 'clabe'], axis=1)\n",
        "y = merged_data['isMatched']\n",
        "\n",
        "# Dividir datos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalizar datos\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Definir el modelo\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(units=64, input_dim=X_train.shape[1], activation='relu'),  # Capa oculta con 64 neuronas y función de activación ReLU\n",
        "    tf.keras.layers.Dropout(0.5), Dropout para regularización\n",
        "    tf.keras.layers.Dense(units=1, activation='sigmoid')  # Capa de salida con 1 neurona y función de activación sigmoide\n",
        "])\n",
        "\n",
        "# Compilar el modelo\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluar el modelo\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Loss: {loss}, Accuracy: {accuracy}')\n",
        "\n",
        "# Hacer predicciones\n",
        "predictions = model.predict(X_test)"
      ]
    }
  ]
}